{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LYgEBEyxIimzD4yu-zG6730huWSx6Rkk",
      "authorship_tag": "ABX9TyNIlGdqXG8tpNHHV8URE2gs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yapjun/anpr_system/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npFjsIZ9EKhL",
        "outputId": "a0c0a634-d321-4e78-cc77-93bc86ca0bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount gog drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download dataset to train YoloV8's number plate detection model"
      ],
      "metadata": {
        "id": "W9IgK4oqIbVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install roboflow\n",
        "\n",
        "# from roboflow import Roboflow\n",
        "# rf = Roboflow(api_key=\"X8uAKKO36QUnYPtGzJcy\")\n",
        "# project = rf.workspace(\"roboflow-universe-projects\").project(\"license-plate-recognition-rxg4e\")\n",
        "# dataset = project.version(4).download(\"yolov8\")\n",
        "\n",
        "# move file to working directory\n",
        "!ls\n",
        "!mkdir datasets\n",
        "!mkdir datasets/License-Plate-Recognition-4/\n",
        "\n",
        "!mv License-Plate-Recognition-4/* datasets/License-Plate-Recognition-4/.\n",
        "!rmdir License-Plate-Recognition-4\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CsgJVzwFnKq",
        "outputId": "a78f7c61-4a21-402a-c5f6-2f506b1b4836"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets  License-Plate-Recognition-4  runs  sample_data  yolov8n.pt\n",
            "mkdir: cannot create directory â€˜datasetsâ€™: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install jupyterlab-lsp\n",
        "# !pip install matplotlib\n",
        "# !pip install Pillow\n",
        "# !pip install pytesseract\n",
        "# !pip install ultralytics==8.0.196\n",
        "# !pip install opencv-python\n",
        "\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "import cv2 # computer vision\n",
        "import pytesseract # OCR tool\n",
        "import os\n",
        "import glob\n",
        "import ultralytics\n",
        "from IPython import display\n",
        "\n",
        "# import YOLO for NPD (and maybe character recognition hehehe...)\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "\n",
        "from tqdm import tqdm # progress bar\n",
        "from PIL import Image # handling images\n",
        "\n",
        "# for web scraping\n",
        "import requests\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# tensorflow keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLpURb3pGUo3",
        "outputId": "27e20a3a-9f00-4833-cc2a-d6846b23d748"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Collecting ultralytics==8.0.196\n",
            "  Downloading ultralytics-8.0.196-py3-none-any.whl (631 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m631.1/631.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (0.16.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (0.13.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.196) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics==8.0.196)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (0.10.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.0.196) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics==8.0.196) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.196) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.196) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.196) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.196) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.196) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from cycler>=0.10->matplotlib>=3.3.0->ultralytics==8.0.196) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.0.196) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics==8.0.196) (1.3.0)\n",
            "Installing collected packages: thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.0.196\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "bYPLdFntRHbM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number Plate Detection using YoloV8's pre-trained model\n"
      ],
      "metadata": {
        "id": "5ADQYZuLHD0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup pre-trained model\n",
        "\n",
        "!ls\n",
        "dataset = \"/content/datasets/License-Plate-Recognition-4/data.yaml\"\n",
        "\n",
        "# load a model\n",
        "# new_backbone = YOLO(\"yolov8n.yaml\") # new model from scratch\n",
        "backbone = YOLO(\"yolov8n.pt\") # load a pre-trained yolo model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grAN2GV2GbHZ",
        "outputId": "ae603449-6074-4f54-c8a3-d25c46858566"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets  runs\tsample_data  yolov8n.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = [4,10,20, 25]\n",
        "count = 10\n",
        "for each in epochs:\n",
        "  results = backbone.train(data=dataset, epochs=each)\n",
        "  # evaluate model's performance on the validation set\n",
        "  results = backbone.val()\n",
        "\n",
        "  # stringers for downloading data\n",
        "  file_name = str(each) + \"epoch.zip\"\n",
        "\n",
        "  directory = \"runs/detect/train\" + str(count)\n",
        "\n",
        "  print(type(directory))\n",
        "  !zip -r ${filename} ${directory}\n",
        "\n",
        "  count = count + 1\n",
        "\n",
        "# perform object detection on an image using the model\n",
        "# results = backbone('path_to_png_for_testing')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qWHfAKrcGgk4",
        "outputId": "356bffa7-2de2-46ec-fd36-231f0c755d1f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/20      2.31G     0.9471     0.3624      1.122          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1324/1324 [06:46<00:00,  3.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:20<00:00,  3.09it/s]\n",
            "                   all       2046       2132      0.978      0.959      0.981      0.696\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/20      2.31G     0.9388     0.3553      1.116          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1324/1324 [07:11<00:00,  3.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:22<00:00,  2.88it/s]\n",
            "                   all       2046       2132      0.979      0.958      0.983      0.701\n",
            "\n",
            "20 epochs completed in 2.415 hours.\n",
            "Optimizer stripped from runs/detect/train10/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train10/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train10/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:30<00:00,  2.07it/s]\n",
            "                   all       2046       2132      0.979      0.957      0.983      0.701\n",
            "Speed: 0.3ms preprocess, 2.3ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train10\u001b[0m\n",
            "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/License-Plate-Recognition-4/valid/labels.cache... 2046 images, 3 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2046/2046 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:27<00:00,  4.71it/s]\n",
            "                   all       2046       2132       0.98      0.957      0.983      0.701\n",
            "Speed: 0.4ms preprocess, 3.9ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "\n",
            "zip error: Invalid command arguments (cannot write zip file to terminal)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "New https://pypi.org/project/ultralytics/8.1.9 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.0.196 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/datasets/License-Plate-Recognition-4/data.yaml, epochs=25, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train7\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 70/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train7', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/License-Plate-Recognition-4/train/labels.cache... 21173 images, 28 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21173/21173 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/License-Plate-Recognition-4/valid/labels.cache... 2046 images, 3 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2046/2046 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train7/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train7\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/25      2.26G     0.9626     0.4322      1.121         16        640:   6%|â–Œ         | 80/1324 [00:27<07:01,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ea0bfaa5cb47>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m# evaluate model's performance on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mni\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_opt_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                     \u001b[0mlast_opt_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mni\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/torch_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ultralytics YOLO ðŸš€, AGPL-3.0 license\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aEbplYFEAC9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# download zippies\n",
        "file_names = [\"4epochs\", \"10epochs\", \"20epochs\"]\n",
        "directory = [\"train8\", \"train9\", \"train10\"]\n",
        "\n",
        "dir_count = 0\n",
        "!ls\n",
        "\n",
        "# !zip -r 4epochs.zip runs/detect/train8/\n",
        "# files.download(\"4epochs.zip\")\n",
        "\n",
        "!zip -r 10epochs.zip runs/detect/train9/\n",
        "\n",
        "!zip -r 20epochs.zip runs/detect/train10/\n",
        "files.download(\"10epochs.zip\")\n",
        "files.download(\"20epochs.zip\")\n",
        "\n",
        "# for each in file_names:\n",
        "#     !zip -r ${each}.zip runs/detect/${directory[dir_count]}\n",
        "#     files.download(str(each + \".zip\"))\n",
        "#     dir_count += 1\n",
        "\n",
        "# pick pre-trained model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WG7PSHp2SazX",
        "outputId": "830a8038-bdb0-44e4-b7e9-7d5786ed2d55"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0epochs.zip  4epochs.zip  datasets  epochs.zip\truns  sample_data  yolov8n.pt\n",
            "  adding: runs/detect/train9/ (stored 0%)\n",
            "  adding: runs/detect/train9/val_batch0_pred.jpg (deflated 7%)\n",
            "  adding: runs/detect/train9/val_batch1_pred.jpg (deflated 7%)\n",
            "  adding: runs/detect/train9/PR_curve.png (deflated 25%)\n",
            "  adding: runs/detect/train9/val_batch2_pred.jpg (deflated 6%)\n",
            "  adding: runs/detect/train9/train_batch0.jpg (deflated 8%)\n",
            "  adding: runs/detect/train9/R_curve.png (deflated 17%)\n",
            "  adding: runs/detect/train9/results.csv (deflated 82%)\n",
            "  adding: runs/detect/train9/val/ (stored 0%)\n",
            "  adding: runs/detect/train9/val/val_batch0_pred.jpg (deflated 7%)\n",
            "  adding: runs/detect/train9/val/val_batch1_pred.jpg (deflated 6%)\n",
            "  adding: runs/detect/train9/val/PR_curve.png (deflated 25%)\n",
            "  adding: runs/detect/train9/val/val_batch2_pred.jpg (deflated 7%)\n",
            "  adding: runs/detect/train9/val/R_curve.png (deflated 18%)\n",
            "  adding: runs/detect/train9/val/confusion_matrix_normalized.png (deflated 35%)\n",
            "  adding: runs/detect/train9/val/val_batch2_labels.jpg (deflated 7%)\n",
            "  adding: runs/detect/train9/val/confusion_matrix.png (deflated 34%)\n",
            "  adding: runs/detect/train9/val/F1_curve.png (deflated 16%)\n",
            "  adding: runs/detect/train9/val/P_curve.png (deflated 20%)\n",
            "  adding: runs/detect/train9/val/val_batch0_labels.jpg (deflated 7%)\n",
            "  adding: runs/detect/train9/val/val_batch1_labels.jpg (deflated 6%)\n",
            "  adding: runs/detect/train9/train_batch2.jpg (deflated 11%)\n",
            "  adding: runs/detect/train9/confusion_matrix_normalized.png (deflated 35%)\n",
            "  adding: runs/detect/train9/labels_correlogram.jpg (deflated 30%)\n",
            "  adding: runs/detect/train9/val_batch2_labels.jpg (deflated 6%)\n",
            "  adding: runs/detect/train9/labels.jpg (deflated 38%)\n",
            "  adding: runs/detect/train9/confusion_matrix.png (deflated 35%)\n",
            "  adding: runs/detect/train9/results.png (deflated 7%)\n",
            "  adding: runs/detect/train9/F1_curve.png (deflated 16%)\n",
            "  adding: runs/detect/train9/P_curve.png (deflated 21%)\n",
            "  adding: runs/detect/train9/val_batch0_labels.jpg (deflated 7%)\n",
            "  adding: runs/detect/train9/events.out.tfevents.1707169110.82db5dd44c3a.448.1 (deflated 76%)\n",
            "  adding: runs/detect/train9/weights/ (stored 0%)\n",
            "  adding: runs/detect/train9/weights/last.pt (deflated 9%)\n",
            "  adding: runs/detect/train9/weights/best.pt (deflated 9%)\n",
            "  adding: runs/detect/train9/args.yaml (deflated 51%)\n",
            "  adding: runs/detect/train9/val_batch1_labels.jpg (deflated 7%)\n",
            "  adding: runs/detect/train9/train_batch1.jpg (deflated 11%)\n",
            "  adding: runs/detect/train10/ (stored 0%)\n",
            "  adding: runs/detect/train10/val_batch0_pred.jpg (deflated 7%)\n",
            "  adding: runs/detect/train10/val_batch1_pred.jpg (deflated 7%)\n",
            "  adding: runs/detect/train10/PR_curve.png (deflated 26%)\n",
            "  adding: runs/detect/train10/val_batch2_pred.jpg (deflated 6%)\n",
            "  adding: runs/detect/train10/train_batch0.jpg (deflated 8%)\n",
            "  adding: runs/detect/train10/R_curve.png (deflated 17%)\n",
            "  adding: runs/detect/train10/results.csv (deflated 84%)\n",
            "  adding: runs/detect/train10/train_batch2.jpg (deflated 6%)\n",
            "  adding: runs/detect/train10/train_batch13240.jpg (deflated 6%)\n",
            "  adding: runs/detect/train10/train_batch13241.jpg (deflated 10%)\n",
            "  adding: runs/detect/train10/confusion_matrix_normalized.png (deflated 35%)\n",
            "  adding: runs/detect/train10/labels_correlogram.jpg (deflated 30%)\n",
            "  adding: runs/detect/train10/val_batch2_labels.jpg (deflated 6%)\n",
            "  adding: runs/detect/train10/labels.jpg (deflated 38%)\n",
            "  adding: runs/detect/train10/confusion_matrix.png (deflated 34%)\n",
            "  adding: runs/detect/train10/results.png (deflated 6%)\n",
            "  adding: runs/detect/train10/F1_curve.png (deflated 16%)\n",
            "  adding: runs/detect/train10/P_curve.png (deflated 22%)\n",
            "  adding: runs/detect/train10/val_batch0_labels.jpg (deflated 7%)\n",
            "  adding: runs/detect/train10/events.out.tfevents.1707173530.82db5dd44c3a.448.2 (deflated 75%)\n",
            "  adding: runs/detect/train10/weights/ (stored 0%)\n",
            "  adding: runs/detect/train10/weights/last.pt (deflated 10%)\n",
            "  adding: runs/detect/train10/weights/best.pt (deflated 10%)\n",
            "  adding: runs/detect/train10/args.yaml (deflated 50%)\n",
            "  adding: runs/detect/train10/train_batch13242.jpg (deflated 10%)\n",
            "  adding: runs/detect/train10/val_batch1_labels.jpg (deflated 7%)\n",
            "  adding: runs/detect/train10/train_batch1.jpg (deflated 9%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_33a96d2b-2865-47e8-b4a4-94c42c4d5635\", \"10epochs.zip\", 21809223)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8012d103-355d-4a3f-bb5b-2c2abe3ab5f3\", \"20epochs.zip\", 19464461)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download training results\n",
        "# !zip -r 2epoch.zip runs/detect/train8\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7f9QyfLQzhf",
        "outputId": "f283fc03-c8c1-4b32-9bf9-751b3846010a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2epoch.zip  datasets  runs  sample_data  yolov8n.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this function sorts the points given into a strucutured format\n",
        "# [(top left), (top right), (bottom right), (bottom left)]\n",
        "def sort_points(points):\n",
        "    # initialize empty array of coordinates\n",
        "    rect = np.zeros((4,2), dtype=\"float32\")\n",
        "\n",
        "    s = points.sum(axis=1)\n",
        "    rect[0] = points[np.argmin(s)] # top left\n",
        "    rect[2] = points[np.argmax(s)] # bottom right\n",
        "\n",
        "    # calculate difference in\n",
        "    diff = np.diff(points, axis=1)\n",
        "    rect[1] = points[np.argmin(diff)] # top right\n",
        "    rect[3] = points[np.argmax(diff)] # bottom left\n",
        "\n",
        "    return rect"
      ],
      "metadata": {
        "id": "3FjZBMbfGn93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Segmentation"
      ],
      "metadata": {
        "id": "QwUlo-sPGvU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unwarp image\n",
        "def transform(image, points):\n",
        "    rect = sort_points(points)\n",
        "    (tl, tr, br, bl) = rect\n",
        "\n",
        "    # compute width of the new image using max distance between the x coordinates of\n",
        "    # bot left and right // top left and right\n",
        "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[2]) ** 2))\n",
        "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[2]) ** 2))\n",
        "    maxWidth = max(int(widthA), int(widthB))\n",
        "\n",
        "    # compute height\n",
        "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[2]) ** 2))\n",
        "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tr[1] - br[2]) ** 2))\n",
        "    maxHeight = max(int(heightA), int(heightB))\n",
        "\n",
        "    # construct top-down view of image\n",
        "    dest_points = np.array([\n",
        "            [0,0],\n",
        "            [maxWidth - 1, 0],\n",
        "            [maxWidth - 1, maxHeight - 1],\n",
        "            [0, maxHeight - 1]], dtype=\"float32\")\n",
        "\n",
        "    # compute perspective transform matrix\n",
        "    matrix = cv2.getPerspectiveTransform(rect, dest_points)\n",
        "    transformed_image = cv2.warpPerspective(image, matrix, (maxWidth, maxHeight))\n",
        "\n",
        "    return transformed_image"
      ],
      "metadata": {
        "id": "X2eKkx34GpL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}